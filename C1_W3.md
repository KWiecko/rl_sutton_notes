### [C1_W3] What are policies and value functions ?

RL considers evaluative feedback to train an agent. Most RL algorithms involve estimation of value functions, described in the book as:

_“ functions of states (or of state-action pars) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state)”_

“how good” — refers to expected return (cumulative future rewards).

Future rewards or expected return depends on the set of actions to be chosen by the agent or ways of acting, referred as policies. As stated in the book:

_“Formally, a policy is a mapping from states to probabilities of selecting each possible action.”_

Policy is represented by mathematical symbol pi and pi(a|s) 
is the probability that A<sub>t</sub> = a if S<sub>t</sub> = s. RL methods inform the change in agent’s policy as it gains more experience.


### [C1_W3] State-value function for policy pi (v<sub>pi</sub>)

The value function for state s with policy pi (v<sub>pi</sub>(s)) is the expected return starting with state s that are followed by actions defined under policy pi.

![state-value-for-policy](pngs/C1_W3_policies.png)


### [C1_W3] Action-value function for policy pi (q<sub>pi</sub>)


The value of taking an action a in state s under policy pi (q<sub>pi</sub>(s,a)) is the expected return starting at state s and action a, and then following policy pi.