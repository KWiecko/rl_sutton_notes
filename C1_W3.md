### [C1_W3] What are policies and value functions ?

<span style="font-size:1.2em;">

RL considers evaluative feedback to train an agent. Most RL algorithms involve estimation of value functions, described in the book as:

 - _“ functions of states (or of state-action pars) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state)”_

“how good” — refers to expected return (cumulative future rewards).

Future rewards or expected return depends on the set of actions to be chosen by the agent or ways of acting, referred as policies. As stated in the book:

 - _“Formally, a policy is a mapping from states to probabilities of selecting each possible action.”_

Policy is represented by mathematical symbol pi and pi(a|s) 
is the probability that A<sub>t</sub> = a if S<sub>t</sub> = s. RL methods inform the change in agent’s policy as it gains more experience.


### [C1_W3] State-value function for policy pi (v<sub>pi</sub>)

The value function for state s with policy pi (v<sub>pi</sub>(s)) is the expected return starting with state s that are followed by actions defined under policy pi.

![state-value-for-policy](pngs/C1_W3_policies.png)


### [C1_W3] Action-value function for policy pi (q<sub>pi</sub>)


The value of taking an action a in state s under policy pi (q<sub>pi</sub>(s,a)) is the expected return starting at state s and action a, and then following policy pi.

![action-value-for-policy1](pngs/C1_W3_policies_1.png)


### [C1_W3] How do we estimate the value functions?


To estimate state-value function under policy pi for state s, v<sub>pi</sub>(s), from experience can involve an agent to find the average of all rewards experienced by the agent followed by state s. If the state s is encountered infinite number of times, then the average will converge to state’s value v<sub>pi</sub>(s).

_“We call estimation methods of this kind Monte Carlo methods because they involve averaging over many random samples of actual returns.”_


### [C1_W3] Recursive relationships of value functions

For computational implementation of RL dynamic programming is crucial, which is executed by establishing recursive relationships for estimations. The recursive relation established earlier in estimation of expected return is used to obtain recursive relation of state-value function for policy pi.

![recursive-value-for-policy1](pngs/C1_W3_policies_2.png)

This is bellman equation for state value function under policy pi that establishes relation between value of a state and value of the next states.

For proof please refer to ![this page](https://stats.stackexchange.com/questions/243384/deriving-bellmans-equation-in-reinforcement-learning)

### [C1_W3] Bellman equation for v<sub>pi</sub>

The Bellman equation averages over all the possibilities, weighting each by its probability of occurring.

The value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.

![recursive-value-for-policy9](pngs/C1_W3_policies_9.png)

![recursive-value-for-policy10](pngs/C1_W3_policies_10.png)

![recursive-value-for-policy2](pngs/C1_W3_policies_3.png)

_“It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way”._

### [C1_W3] Optimality (Optimal Policies & Optimal Value Functions)

To solve a RL problem, we aim to find the optimal policy — policy that provides maximum expected return in long run. A policy pi is considered better than or equal to policy pi’ if the expected return for policy pi is greater than or equal to that for policy pi’ for all states. Optimal policy is the policy that is better than or equal to any of the other policies for all states (denoted as pi<sub>*</sub>). The optimal state-value function v<sub>*</sub>(s) is

![recursive-value-for-policy3](pngs/C1_W3_policies_4.png)

There is always at least one policy that is better than or equal to all other policies.

If there are more than one optimal policies, they share the same optimal state-value function and the same optimal action-value function (q_{*}(s,a)), defined below.

![recursive-value-for-policy4](pngs/C1_W3_policies_5.png)

Note that q<sub>*</sub> can be expressed in terms of v<sub>*</sub>, since q<sub>*</sub> represents the expected return after taking action a at the starting state s and following optimal policy.

![recursive-value-for-policy5](pngs/C1_W3_policies_6.png)

v<sub>*</sub> follows the Bellman equation for state values defined above.

### [C1_W3] Bellman optimality equation for v_{*}

Under an optimal policy, the value of a state, must be the expected return for the optimal action at that state.

![recursive-value-for-policy6](pngs/C1_W3_policies_7.png)

Bellman optimality equation for q<sub>*</sub>

![recursive-value-for-policy7](pngs/C1_W3_policies_8.png)

![recursive-value-for-policy11](pngs/C1_W3_policies_11.png)

Bellman Optimality equation relies one at least three assumptions that are rarely true in practice.

 - Accurately know the dynamics of the environment.
 - Have enough computational resources
 - Markov Property.

### [C1_W3] Specifying policies

![recursive-value-for-policy12](pngs/C1_W3_policies_12.png)

![recursive-value-for-policy13](pngs/C1_W3_policies_13.png)

![recursive-value-for-policy14](pngs/C1_W3_policies_14.png)

sum of all the probabilities has to be 1

![recursive-value-for-policy15](pngs/C1_W3_policies_15.png)

![recursive-value-for-policy17](pngs/C1_W3_policies_17.png)

![recursive-value-for-policy18](pngs/C1_W3_policies_18.png)

![recursive-value-for-policy19](pngs/C1_W3_policies_19.png)

In games like chess and the above, there is spare rewards because reward is only given when somebody wins and when the agent moves to A or B.

In this kind of cases, value function is pretty crucial.

 - the reason why value function of state A is lower than 10 is because after A moves to A’, there are more possibilities to bump and get negative reward.
 - and the state of B has larger value than the reward because after it moves to B’, there is no such next state to get negative rewards.

### [C1_W3] How do Bellman optimality equations help solve the RL problem?

For n states, there are n bellman optimality equations, one for each state, and n unknows (optimal value for each state — v_{*}). When the dynamic p of a MDP is given, the values of v_{*} for each state can be found by solving the system of linear equations. Once we have v_{*}, then the policy that gives nonzero probability to the actions with maximum value at all states can be identified as the optimal policy.

_“If you have the optimal value function, v_{*}, then the actions that appear best after a one-step search will be optimal actions.”_

_“any policy that is greedy with respect to the optimal evaluation function v_{*} is an optimal policy”_

The most important and elegant aspect of Bellman optimality equations is that with v_{*} “the optimal expected long-term return” is “locally and immediately available for each state”.

### [C1_W3] Summary

#### [C1_W3] State-value Bellman Equation

![recursive-value-for-policy20](pngs/C1_W3_policies_20.png)

![recursive-value-for-policy21](pngs/C1_W3_policies_21.png)

One of the points is that it multiplies with the probability of next state

for the expected return, it sums all the action

#### [C1-W3] Action-value Bellman equation

![recursive-value-for-policy22](pngs/C1_W3_policies_22.png)

at this time, for the expected return, it has to compute each actions and multiply with the probability and sum it all.

#### [C1_W3] State-value Bellman equation - example

![recursive-value-for-policy23](pngs/C1_W3_policies_23.png)

![recursive-value-for-policy24](pngs/C1_W3_policies_24.png)

#### [C1_W3] Optimal policy with discount gamma

![recursive-value-for-policy25](pngs/C1_W3_policies_25.png)

![recursive-value-for-policy26](pngs/C1_W3_policies_26.png)

The optimal policy has to have the highest value at all the states

![recursive-value-for-policy27](pngs/C1_W3_policies_27.png)

Bellman Optimality Equation is always picking the highest value’s action at each state like the graph above.

![recursive-value-for-policy28](pngs/C1_W3_policies_28.png)

![recursive-value-for-policy29](pngs/C1_W3_policies_29.png)

![recursive-value-for-policy30](pngs/C1_W3_policies_30.png)

we don’t know what’s the optimal policy yet so we can’t follow optimal policy with Linear system solver.

we have to find the optimal policy by doing max a.

![recursive-value-for-policy31](pngs/C1_W3_policies_31.png)

So the maximum value will be 10 and the optimal action will be A2.

![recursive-value-for-policy32](pngs/C1_W3_policies_32.png)

![recursive-value-for-policy33](pngs/C1_W3_policies_33.png)

![recursive-value-for-policy34](pngs/C1_W3_policies_34.png)

![recursive-value-for-policy35](pngs/C1_W3_policies_35.png)

![recursive-value-for-policy36](pngs/C1_W3_policies_36.png)

![recursive-value-for-policy37](pngs/C1_W3_policies_37.png)

The difference between the state value function and action value function is that the state value function has to be computed with the one-step dynamics of the MDP.

However, much less work is required if we have the optimal action value function.

![recursive-value-for-policy38](pngs/C1_W3_policies_38.png)

for the state action value function, it’s easier to say that even though the agent take a certain same action, the state might be changed differently. Therefore, it multiplies the probability of state transition.

</span>